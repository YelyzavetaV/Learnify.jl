using Statistics: mean
using LinearAlgebra: norm

"""
    enet!(Œ∏::Vector{F}, X::Matrix{F}, y::Vector{F}, Œ±, Œ≤;
        mit::Integer=1000, œµ=1e-5)

Elastic Net regression method.

# References
[1] S.-J. Kim et al. "An Interior-Point Method for Large-Scale l‚ÇÅ-Regularized Least
    Squares". IEEE J. Sel. Top. Signal Process. 1(4):606-617, 2007.
[2] F. Pedregosa et. al. "Scikit-learn: Machine Learning in Python". J. Mach. Learn. Res.
    12:2825-2830, 2011.
"""
function enet!(Œ∏::Vector{F}, X::Matrix{F}, y::Vector{F}, Œ±, Œ≤;
    mit::Integer=1000, œµ=1e-5) where {F}

    M, N = size(X, 1), size(X, 2)
    Œª = Œ± * Œ≤ * M
    Œ≥ = Œ± * (1.0 - Œ≤) * M

    R = y - X * Œ∏
    z = sum(X .^ 2, dims=1)

    œµÃÉ = œµ * y' * y
    Œ∑ = œµÃÉ + 1.0

    it = 0
    converged = false
    while (~converged && it < mit)
        it += 1
        Œ∏‚Çò, ùùô = 0.0, 0.0
        for j = 1:N
            z[j] == 0 && continue
            Œ∏ÃÉ = Œ∏[j]
            if Œ∏ÃÉ ‚â† 0
                R += X[:, j] * Œ∏ÃÉ
            end
            G = X[:, j]' * R
            Œ∏[j] = sign(G) * max(abs(G) - Œª, 0) / (z[j] + Œ≥)
            if Œ∏[j] ‚â† 0
                R -= X[:, j] * Œ∏[j]
            end
            ùùô = max(ùùô, abs(Œ∏[j] - Œ∏ÃÉ))
            Œ∏‚Çò = max(Œ∏‚Çò, abs(Œ∏[j]))
        end
        if (Œ∏‚Çò == 0 || ùùô / Œ∏‚Çò < œµ || it == mit)
            c‚ÇÅ = c‚ÇÇ = c‚ÇÉ = 1
            d = maximum(abs.(X' * R - Œ≥ * Œ∏))
            if d > Œª
                s = Œª / d
                c‚ÇÅ -= 0.5 * (1.0 - s^2)
                c‚ÇÇ = c‚ÇÉ = s
            end
            Œ∑ = (
                c‚ÇÅ * R' * R
                - c‚ÇÇ * R' * y
                + Œª * norm(Œ∏, 1)
                + 0.5 * Œ≥ * (1.0 + c‚ÇÉ^2) * (Œ∏' * Œ∏)
            )
            if Œ∑ < œµÃÉ
                converged = true
            end
        end
    end
    if ~converged
        @warn """Elastic Net algorithm did not converge: try increasing the number of
            maximal allowed iterations mit or decreasing the tolerance œµ."""
    end
    return it, Œ∑
end

function preprocess!(X::M, y::V, intercept::Bool=true) where {M<:AbstractMatrix, V<:AbstractVector}
    XÃÑ = zeros(eltype(X), size(X, 2))
    yÃÑ = zeros(eltype(y), size(y, 1))
    if intercept
        XÃÑ = mean(X, dims=1)
        yÃÑ = mean(y)
        X .-= XÃÑ
        y .-= yÃÑ
    end
    return XÃÑ, yÃÑ
end

"""
    ElasticNet(
        X::Matrix{F},
        y::Vector{F};
        Œ±=1.0,
        Œ≤=0.5,
        intercept::Bool=true,
        mit::Integer=1000,
        œµ=1e-5,
    )

Elastic Net regression model that solves the optimization problem
    min[R(Œ∏)] = min[1/(2M)‚Äñy - XŒ∏‚Äñ¬≤ + Œ±Œ≤‚ÄñŒ∏‚Äñ‚ÇÅ + (1/2)Œ±(1 - Œ≤)‚ÄñŒ∏‚Äñ¬≤],
where X and y are the training data with M samples and N features, and ‚Äñ‚ãÖ‚Äñ‚ÇÅ and ‚Äñ‚ãÖ‚Äñ
denote l‚ÇÅ and l‚ÇÇ norms, respectively.
If `intercept` is true, training data will be centered.
`mit` is a maximal number of iterations and `œµ` is a convergence tolerance for the
duality gap.

# References
[1] J. Friedman et al. "Regularization Paths for Generalized Linear Models via Coordinate
    Descent". J. Stat. Softw. 33(1):1-22, 2010.
"""
mutable struct ElasticNet{F} <: LinearRegression
    I::F
    Œ∏::Vector{F}
    N·µ¢::Integer
    Œ∑
    function ElasticNet(
        X::Matrix{F},
        y::Vector{F};
        Œ±=1.0,
        Œ≤=0.5,
        intercept::Bool=true,
        mit::Integer=1000,
        œµ=1e-5,
    ) where {F}
        XÃÑ, yÃÑ = preprocess!(X, y, intercept)
        Œ∏ = zeros(F, size(X, 2))

        N·µ¢, Œ∑ = enet!(Œ∏, X, y, Œ±, Œ≤; mit, œµ)

        I = (yÃÑ .- XÃÑ * Œ∏)[1]

        new{F}(I, Œ∏, N·µ¢, Œ∑)
    end

end

function (m::ElasticNet)(pts::AbstractVecOrMat)
    pts * m.Œ∏ .+ m.I
end

"""
"""
function Lasso(
    X::Matrix{F},
    y::Vector{F};
    Œ±=1.0,
    intercept::Bool=true,
    mit::Integer=1000,
    œµ=1e-5,
) where {F}
    ElasticNet(X, y; Œ±=Œ±, Œ≤=1.0, intercept=intercept, mit=mit, œµ=œµ)
end
